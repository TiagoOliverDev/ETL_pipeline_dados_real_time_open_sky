# ğŸŒ¦ï¸ Natal METEO API Pipeline

Pipeline ETL para coleta, processamento e armazenamento de dados meteorolÃ³gicos de Natal, utilizando **Python**, **Apache Airflow** e **PostgreSQL**.

---

## ğŸ“š VisÃ£o Geral

- **ExtraÃ§Ã£o:** Coleta dados da API METEO e salva em JSON.
- **TransformaÃ§Ã£o:** Normaliza, limpa e transforma os dados em CSV.
- **Carga:** Insere os dados processados em um banco PostgreSQL.
- **OrquestraÃ§Ã£o:** Todo o fluxo Ã© automatizado via Apache Airflow.

---

## ğŸ—‚ï¸ Estrutura do Projeto

```bash
api_meteo_pipeline/
â”œâ”€â”€ config/                  # ConfiguraÃ§Ãµes globais do pipeline
â”‚   â””â”€â”€ settings.py
â”œâ”€â”€ dags/                    # DAGs do Airflow
â”‚   â””â”€â”€ city_weather_dag.py
â”œâ”€â”€ data/                    # Dados brutos e processados (ignorado pelo git)
â”‚   â”œâ”€â”€ raw/
â”‚   â””â”€â”€ processed/
â”œâ”€â”€ logs/                    # Logs do Airflow (ignorado pelo git)
â”œâ”€â”€ notebooks/               # Notebooks de anÃ¡lise e exploraÃ§Ã£o
â”œâ”€â”€ plugins/                 # Plugins customizados do Airflow (opcional)
â”œâ”€â”€ src/     
â”‚   â”œâ”€â”€ db/                  # Arquivo de conexÃ£o no banco dinÃ¢mico (Pode conectar localmente via variÃ¡veis .env ou pode conectar no banco do Airflow via connections id)
â”‚   â”‚   â””â”€â”€ db_connections.py      
â”‚   â”œâ”€â”€ etl/                 # Aonde a mÃ¡gica acontece, aqui fica o ETL do projeto
â”‚   â”‚   â””â”€â”€ extract_data.py
â”‚   â”‚   â””â”€â”€ json_loader.py
â”‚   â”‚   â””â”€â”€ load_data.py
â”‚   â”‚   â””â”€â”€ transform_data.py
â”‚   â”œâ”€â”€ utils/               # Aqui fica algumas funÃ§Ãµes Ãºteis
â”‚   â”‚   â””â”€â”€ logger.py
â”œâ”€â”€ tests/                   # Testes automatizados
â”œâ”€â”€ .env.example             # Exemplo de VariÃ¡veis de ambiente 
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .dockerignore
â”œâ”€â”€ docker-compose.yml       # OrquestraÃ§Ã£o dos containers
â”œâ”€â”€ Dockerfile               # Build customizado (opcional)
â”œâ”€â”€ requirements.txt         # DependÃªncias Python
â””â”€â”€ main.py                  # ExecuÃ§Ã£o manual do pipeline (fora do Airflow)

```

---

## âš™ï¸ ConfiguraÃ§Ã£o do Ambiente

### 1. **PrÃ©-requisitos**

- [Docker](https://www.docker.com/)
- [Docker Compose](https://docs.docker.com/compose/)
- Python 3.11+ (apenas para execuÃ§Ã£o manual)

```
Crie um banco de dados local para acompanhar os dados caindo, insira as credÃªnciais dele no .env e crie a tabela:

CREATE TABLE IF NOT EXISTS natal_weather_records (
    id SERIAL PRIMARY KEY,
    measurement_datetime TIMESTAMP WITHOUT TIME ZONE NOT NULL,
    interval_seconds INTEGER,
    temperature_celsius NUMERIC(5,2) NOT NULL,
    wind_speed_kmh NUMERIC(5,2) NOT NULL,
    wind_direction_degrees INTEGER NOT NULL,
    is_day INTEGER,
    weather_condition_code INTEGER NOT NULL,
    latitude NUMERIC(8,5) NOT NULL,
    longitude NUMERIC(8,5) NOT NULL,
    timezone VARCHAR(50) NOT NULL,
    elevation INTEGER NOT NULL,
    record_timestamp TIMESTAMP WITHOUT TIME ZONE NOT NULL
);


```

### 2. **Clone o RepositÃ³rio**

```bash
git clone https://github.com/TiagoOliverDev/ETL_Pipeline_de_Dados_Climaticos
cd ETL_Pipeline_de_Dados_Climaticos
```

### 3. **Crie o Arquivo `.env`**

Baseie-se no modelo .env.example

### 4. **Suba o Ambiente com Docker Compose**

```bash
docker-compose up --build
```

- O Airflow estarÃ¡ disponÃ­vel em: [http://localhost:8080](http://localhost:8080/)
- UsuÃ¡rio e senha definidos no `.env`

### 5. **Ative a DAG no Airflow**

1. Acesse a interface web do Airflow
2. Ative a DAG `natal_weather_pipeline`
3. VocÃª pode disparar manualmente ou aguardar a execuÃ§Ã£o automÃ¡tica

---

## ğŸ› ï¸ ExecuÃ§Ã£o Manual (fora do Airflow)

Se desejar rodar o pipeline manualmente:

```bash
python main.py
```

---

## ğŸ§© Principais Arquivos

- `dags/city_weather_dag.py`: DAG principal do Airflow
- `src/etl/extract_data.py`: FunÃ§Ã£o de extraÃ§Ã£o da API
- `src/etl/transform_data.py`: FunÃ§Ãµes de transformaÃ§Ã£o
- `src/etl/load_data.py`: FunÃ§Ã£o de carga no PostgreSQL
- `config/settings.py`: ParÃ¢metros globais do pipeline
- `main.py`: ExecuÃ§Ã£o manual do pipeline

---

## ğŸ“ ObservaÃ§Ãµes

- Os diretÃ³rios `data/raw`, `data/processed` e `logs` estÃ£o no `.gitignore`
- A tabela Ã© criada automaticamente no banco se nÃ£o existir
- O Airflow usa a conexÃ£o nomeada `postgres_etl_conn`

---

## ğŸ“š LicenÃ§a

Este projeto Ã© open-source e estÃ¡ sob a licenÃ§a MIT.

---

## ğŸ‘©â€ğŸ’» Desenvolvido por

Tiago Oliveira â€“ Analista desenvolvedor e Engenheiro de dados em formaÃ§Ã£o

- ğŸ’¼Â [LinkedIn](https://www.linkedin.com/in/tiago-oliveira-49a2a6205/)
- ğŸ’»Â [GitHub](https://github.com/TiagoOliverDev)

