# ‚úàÔ∏è Flight Data Global Pipeline

[![MIT License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)
[![Airflow DAG](https://img.shields.io/badge/Airflow-DAG-blue)](http://localhost:8080)
[![PostgreSQL](https://img.shields.io/badge/PostgreSQL-‚úîÔ∏è-blue)](https://www.postgresql.org/)
[![Kafka + Spark Streaming](https://img.shields.io/badge/Streaming-Kafka%20%2B%20Spark-orange)](https://spark.apache.org/structured-streaming/)

Pipeline ETL e de **streaming** para coleta, processamento e armazenamento de dados de voos em tempo real, utilizando **Python**, **Apache Airflow**, **Apache Kafka**, **Apache Spark Structured Streaming** e **PostgreSQL**.

---

## üìö Vis√£o Geral

- **Extra√ß√£o (batch e streaming):**
  - Batch: coleta peri√≥dica via DAG no Airflow a partir da API OpenSky Network.
  - Streaming: dados produzidos continuamente em t√≥picos Kafka.
- **Transforma√ß√£o:** dados normalizados, limpos e enriquecidos via pandas (batch) ou Spark Structured Streaming (tempo real).
- **Carga:** os dados s√£o inseridos ou atualizados no banco PostgreSQL.
- **Orquestra√ß√£o:** Airflow gerencia tarefas em lote; Kafka + Spark processam o fluxo cont√≠nuo.

---

## üóÇÔ∏è Estrutura do Projeto

```bash
ETL_pipeline_dados_real_time_open_sky/
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îî‚îÄ‚îÄ settings.py
‚îú‚îÄ‚îÄ dags/
‚îÇ   ‚îî‚îÄ‚îÄ flight_data_dag.py
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/
‚îÇ   ‚îî‚îÄ‚îÄ processed/
‚îú‚îÄ‚îÄ logs/
‚îú‚îÄ‚îÄ notebooks/
‚îú‚îÄ‚îÄ plugins/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ db/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ db_connections.py
‚îÇ   ‚îú‚îÄ‚îÄ etl/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ extract_data.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ json_loader.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ load_data.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ transform_data.py
‚îÇ   ‚îú‚îÄ‚îÄ streaming/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ kafka_consumer.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ structured_streaming.py
‚îÇ   ‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ logger.py
‚îú‚îÄ‚îÄ tests/
‚îú‚îÄ‚îÄ .env.example
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ .dockerignore
‚îú‚îÄ‚îÄ docker-compose.yml
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ main.py
```

---

## ‚öôÔ∏è Arquitetura do Processamento de Streaming

```bash
+------------------+        +------------------+        +--------------------------+
|     Producer     | -----> |   Kafka Topic    | -----> | Spark Structured Stream  |
| (OpenSky + ETL)  | JSON   | flight-data-raw  |        |  Transform + PostgreSQL  |
+------------------+        +------------------+        +--------------------------+
                                                    ‚îî‚îÄ>  UPSERT (merge/update)
```

---

## ‚öôÔ∏è Configura√ß√£o do Ambiente

### 1. **Pr√©-requisitos**

- [Docker](https://www.docker.com/)
- [Docker Compose](https://docs.docker.com/compose/)
- Python 3.11+ (para execu√ß√µes locais e testes)

Crie um banco PostgreSQL e insira as credenciais no `.env`. Use o seguinte esquema:

```sql
CREATE TABLE IF NOT EXISTS flight_data (
    icao24 VARCHAR(10) NOT NULL,
    callsign VARCHAR(10),
    origin_country VARCHAR(100),
    latitude DOUBLE PRECISION,
    longitude DOUBLE PRECISION,
    velocity DOUBLE PRECISION,
    heading DOUBLE PRECISION,
    baro_altitude DOUBLE PRECISION,
    geo_altitude DOUBLE PRECISION,
    on_ground BOOLEAN,
    time_position TIMESTAMP WITHOUT TIME ZONE NOT NULL,
    last_contact TIMESTAMP WITHOUT TIME ZONE,
    record_timestamp TIMESTAMP WITHOUT TIME ZONE,
    PRIMARY KEY (icao24, time_position)
);
```

---

### 2. **Clone o Reposit√≥rio**

```bash
git clone https://github.com/TiagoOliverDev/ETL_Pipeline_de_Dados_Climaticos
cd ETL_Pipeline_de_Dados_Climaticos
```

---

### 3. **Crie o Arquivo `.env`**

Baseie-se no modelo `.env.example`.

---

### 4. **Suba o Ambiente com Docker Compose**

```bash
docker-compose up --build
```

- Airflow: [http://localhost:8080](http://localhost:8080)
- Kafka: padr√£o porta 9092
- PostgreSQL: conforme definido no `.env`

---

### 5. **Ative a DAG no Airflow**

1. Acesse a interface web do Airflow
2. Ative a DAG `flight_data_pipeline`
3. Configure a conex√£o `postgres_etl_conn` com seu banco
4. Dispare manualmente ou aguarde execu√ß√£o autom√°tica

---

## üõ†Ô∏è Execu√ß√£o Manual (fora do Airflow)

```bash
python main.py
```

Para execu√ß√£o do consumidor Spark Streaming (caso n√£o esteja orquestrado via DAG):

```bash
spark-submit src/streaming/spark_streaming_consumer.py
```

---

## üß© Principais Arquivos

| Caminho | Descri√ß√£o |
|--------|-----------|
| `dags/flight_data_dag.py` | DAG principal (Airflow) |
| `src/etl/extract_data.py` | Coleta dados da OpenSky API |
| `src/etl/transform_data.py` | Limpa e normaliza os dados |
| `src/etl/load_data.py` | Carga no PostgreSQL |
| `src/streaming/kafka_producer.py` |Envia dados em forma de mensagem na fila a cada 60 segundos |
| `src/streaming/structured_streaming.py` | Consome dados do Kafka e grava no banco |
| `config/settings.py` | Configura√ß√µes globais |
| `main.py` | Execu√ß√£o local (batch) |

---

## üìù Observa√ß√µes

- Os diret√≥rios `data/raw`, `data/processed` e `logs` est√£o no `.gitignore`
- A tabela `flight_data` √© criada automaticamente se n√£o existir
- Airflow usa a conex√£o `postgres_etl_conn`
- Kafka pode ser executado via Docker ou servi√ßo externo

---

## üìö Licen√ßa

Este projeto √© open-source e est√° sob a licen√ßa [MIT](LICENSE).

---

## üë®‚Äçüíª Desenvolvido por

**Tiago Oliveira**  
Analista desenvolvedor e Engenheiro de dados em forma√ß√£o

- üíº [LinkedIn](https://www.linkedin.com/in/tiago-oliveira-49a2a6205/)
- üíª [GitHub](https://github.com/TiagoOliverDev)